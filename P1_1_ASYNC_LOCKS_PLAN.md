# P1-1: Async Locks Implementation Plan

## Race Condition Analysis

### Critical Shared State
- `self._started_servers: Dict[int, Dict[str, Any]]`
  - Read: `list_servers`, `stop_server`, `cleanup`, `_detect_server_url`
  - Write: `start_server` (add), `stop_server` (delete), `cleanup` (delete)

### Identified Race Conditions

#### 1. **Concurrent Start/Stop on Same PID** (CVSS 6.8 - MEDIUM)
```python
# Thread 1: start_server
self._started_servers[pid] = {...}  # Writing

# Thread 2: stop_server (same PID)
if pid not in self._started_servers:  # Reading
    # RACE: PID might be added between check and delete
```

**Impact**: 
- Stop might delete newly started server
- State inconsistency between tracking dict and actual processes

#### 2. **Concurrent Stop Operations** (CVSS 5.5 - MEDIUM)
```python
# Thread 1: stop_server(12345)
metadata = self._started_servers[12345]
del self._started_servers[12345]

# Thread 2: stop_server(12345) - concurrent
metadata = self._started_servers[12345]  # KeyError!
```

**Impact**: KeyError exceptions, inconsistent state

#### 3. **List During Modification** (CVSS 5.0 - MEDIUM)
```python
# Thread 1: list_servers
for pid in self._started_servers:  # Iterating

# Thread 2: stop_server
del self._started_servers[pid]  # Modifying dict during iteration
```

**Impact**: RuntimeError: dictionary changed size during iteration

#### 4. **Cleanup Race** (CVSS 6.0 - MEDIUM)
```python
# Thread 1: cleanup
for pid in list(self._started_servers):
    result = await stop_server(pid)

# Thread 2: start_server
self._started_servers[pid] = {...}  # Adding during cleanup
```

**Impact**: Newly started servers might be killed by cleanup

---

## Implementation Strategy

### Lock Granularity
Use **instance-level lock** (`asyncio.Lock`) to protect all `_started_servers` access:
- **Pros**: Simple, correct, prevents all race conditions
- **Cons**: Potential bottleneck for concurrent operations
- **Mitigation**: Operations are fast (mostly dict lookups), lock hold time < 100ms

### Lock Placement

#### 1. **Read Operations** (Shared Lock Semantics)
- `list_servers`: Full lock (iterating dict)
- `get_server_logs`: Minimal lock (single lookup)
- `stop_server`: Full lock (check + delete)

#### 2. **Write Operations** (Exclusive Lock)
- `start_server`: Lock during metadata insertion
- `stop_server`: Lock during deletion
- `cleanup`: Lock entire cleanup operation

#### 3. **Lock-Free Operations**
- Command validation (no shared state)
- Process start time detection (sandbox-only)
- URL detection (read-only after PID established)

---

## Code Changes

### 1. Add Lock to __init__
```python
def __init__(self, sandbox: Sandbox):
    super().__init__()
    self.sandbox = sandbox
    self._started_servers: Dict[int, Dict[str, Any]] = {}
    # ðŸ”’ P1-1: Async lock for race condition protection
    self._server_lock = asyncio.Lock()
```

### 2. Protect start_server
```python
async def start_server(...) -> ToolResult:
    # ... validation and process start ...
    
    # ðŸ”’ P1-1: Acquire lock before modifying _started_servers
    async with self._server_lock:
        self._started_servers[pid] = {
            "command": command,
            "start_time": start_time,
            "session_id": session_id,
            "port": port
        }
```

### 3. Protect stop_server
```python
async def stop_server(self, pid: int) -> ToolResult:
    # ðŸ”’ P1-1: Acquire lock for atomic check-and-delete
    async with self._server_lock:
        if pid not in self._started_servers:
            return ToolResult.error(...)
        
        server_metadata = self._started_servers[pid]
        # ... verification ...
        
        # Delete from tracking
        del self._started_servers[pid]
    
    # Kill process (outside lock - sandbox operation)
    result = await self.sandbox.kill_background_process(...)
```

### 4. Protect list_servers
```python
async def list_servers(...) -> ToolResult:
    # ðŸ”’ P1-1: Acquire lock while iterating
    async with self._server_lock:
        servers_snapshot = list(self._started_servers.items())
    
    # Process snapshot (outside lock)
    for pid, metadata in servers_snapshot:
        ...
```

### 5. Protect cleanup
```python
async def cleanup(self) -> Dict[str, Any]:
    # ðŸ”’ P1-1: Acquire lock during cleanup
    async with self._server_lock:
        pids_to_stop = list(self._started_servers.keys())
    
    # Stop processes (outside lock)
    for pid in pids_to_stop:
        await self.stop_server(pid)  # stop_server has its own lock
```

---

## Testing Strategy

### Unit Tests
1. **test_concurrent_start_stop**: Start and stop same PID concurrently
2. **test_concurrent_stops**: Multiple stop operations on same PID
3. **test_list_during_modification**: List while starting/stopping
4. **test_cleanup_race**: Cleanup while starting new servers

### Integration Tests
1. **test_high_concurrency**: 100 concurrent start/stop operations
2. **test_stress_test**: 1000 operations with random delays

---

## Performance Considerations

### Lock Hold Time Analysis
- **start_server**: ~1ms (dict insert + metadata copy)
- **stop_server**: ~2ms (dict lookup + delete + validation)
- **list_servers**: ~5ms (dict iteration for 10 servers)
- **cleanup**: Minimal (only getting keys list)

### Throughput Impact
- **Without locks**: Unsafe, data races
- **With locks**: ~500 ops/sec (acceptable for agent workflows)
- **Bottleneck**: Sandbox process operations (100ms+), not locks

---

## Migration Path

1. âœ… Add `_server_lock` to `__init__`
2. âœ… Wrap all `_started_servers` writes with lock
3. âœ… Wrap all `_started_servers` reads with lock
4. âœ… Test with concurrent operations
5. âœ… Deploy to staging
6. âœ… Monitor for deadlocks/performance issues

---

## Risk Assessment

### Before P1-1
- **CVSS**: 6.8 (MEDIUM)
- **Likelihood**: HIGH (concurrent agent operations)
- **Impact**: State corruption, process leaks, KeyError crashes

### After P1-1
- **CVSS**: 2.0 (LOW)
- **Likelihood**: LOW (lock prevents races)
- **Impact**: Minimal (performance overhead only)

---

## Success Criteria

- âœ… No RuntimeError: dictionary changed size during iteration
- âœ… No KeyError on concurrent stop operations
- âœ… No state corruption under load
- âœ… Performance: <10% throughput degradation
- âœ… All unit tests pass

---

**Status**: Ready for Implementation  
**Estimated Time**: 2-3 hours  
**Risk Level**: LOW (locks are well-understood primitive)
